@misc{huang2020tabtransformertabulardatamodeling,
      title={TabTransformer: Tabular Data Modeling Using Contextual Embeddings}, 
      author={Xin Huang and Ashish Khetan and Milan Cvitkovic and Zohar Karnin},
      year={2020},
      eprint={2012.06678},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2012.06678}, 
}

@misc{gorishniy2023revisitingdeeplearningmodels,
      title={Revisiting Deep Learning Models for Tabular Data}, 
      author={Yury Gorishniy and Ivan Rubachev and Valentin Khrulkov and Artem Babenko},
      year={2023},
      eprint={2106.11959},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2106.11959}, 
}

@article{Lee_2019,
   title={BioBERT: a pre-trained biomedical language representation model for biomedical text mining},
   volume={36},
   ISSN={1367-4811},
   url={http://dx.doi.org/10.1093/bioinformatics/btz682},
   DOI={10.1093/bioinformatics/btz682},
   number={4},
   journal={Bioinformatics},
   publisher={Oxford University Press (OUP)},
   author={Lee, Jinhyuk and Yoon, Wonjin and Kim, Sungdong and Kim, Donghyeon and Kim, Sunkyu and So, Chan Ho and Kang, Jaewoo},
   editor={Wren, Jonathan},
   year={2019},
   month=sep, pages={1234–1240} }

@misc{ling2023bioclinicalbertbertbase,
      title={Bio+Clinical BERT, BERT Base, and CNN Performance Comparison for Predicting Drug-Review Satisfaction}, 
      author={Yue Ling},
      year={2023},
      eprint={2308.03782},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2308.03782}, 
}

@misc{devlin2019bertpretrainingdeepbidirectional,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1810.04805}, 
}

@misc{Doshi_2025, 
      title={Gemini 2.5: Our most intelligent models are getting even better}, 
      url={https://blog.google/technology/google-deepmind/google-gemini-updates-io-2025/#developer-experience}, 
      journal={Google}, 
      publisher={Google}, 
      author={Doshi, Tulsee}, 
      year={2025}, 
      month={May}
}

@misc{xGrokBeta,
	author = {XAI},
	title = {{G}rok 3 {B}eta — {T}he {A}ge of {R}easoning {A}gents | x{A}{I} --- x.ai},
	url= {https://x.ai/news/grok-3},
      publisher = {xAI},
	year = {2025},
      month = {Feb}
}

@misc{claude4,
  author = {Anthropic},
  month = {May},
  title = {System Card: Claude Opus 4 \& Claude Sonnet 4},
  url = {https://www-cdn.anthropic.com/4263b940cabb546aa0e3283f35b686f4f3b2ff47.pdf},
  urldate = {2025-05-24},
  year = {2025}
}

@misc{2025geminipro,
  author = {Google DeepMind},
  month = {05},
  title = {Gemini 2.5 Pro Preview Model Card},
  url = {https://storage.googleapis.com/model-cards/documents/gemini-2.5-pro-preview.pdf},
  year = {2025}
}

@misc{yang2025qwen3technicalreport,
      title={Qwen3 Technical Report}, 
      author={An Yang and Anfeng Li and Baosong Yang and Beichen Zhang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Gao and Chengen Huang and Chenxu Lv and Chujie Zheng and Dayiheng Liu and Fan Zhou and Fei Huang and Feng Hu and Hao Ge and Haoran Wei and Huan Lin and Jialong Tang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Yang and Jiaxi Yang and Jing Zhou and Jingren Zhou and Junyang Lin and Kai Dang and Keqin Bao and Kexin Yang and Le Yu and Lianghao Deng and Mei Li and Mingfeng Xue and Mingze Li and Pei Zhang and Peng Wang and Qin Zhu and Rui Men and Ruize Gao and Shixuan Liu and Shuang Luo and Tianhao Li and Tianyi Tang and Wenbiao Yin and Xingzhang Ren and Xinyu Wang and Xinyu Zhang and Xuancheng Ren and Yang Fan and Yang Su and Yichang Zhang and Yinger Zhang and Yu Wan and Yuqiong Liu and Zekun Wang and Zeyu Cui and Zhenru Zhang and Zhipeng Zhou and Zihan Qiu},
      year={2025},
      eprint={2505.09388},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2505.09388}, 
}

@misc{lundberg2017unifiedapproachinterpretingmodel,
      title={A Unified Approach to Interpreting Model Predictions}, 
      author={Scott Lundberg and Su-In Lee},
      year={2017},
      eprint={1705.07874},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/1705.07874}, 
}

@misc{liu2019robertarobustlyoptimizedbert,
      title={RoBERTa: A Robustly Optimized BERT Pretraining Approach}, 
      author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
      year={2019},
      eprint={1907.11692},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1907.11692}, 
}

@misc{clark2020electrapretrainingtextencoders,
      title={ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators}, 
      author={Kevin Clark and Minh-Thang Luong and Quoc V. Le and Christopher D. Manning},
      year={2020},
      eprint={2003.10555},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2003.10555}, 
}